{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "%matplotlib widget\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "\n",
    "import scipy.io as sio\n",
    "import logging\n",
    "from transformers import GPT2Model, GPT2Tokenizer, GPT2Config\n",
    "from collections import namedtuple, defaultdict\n",
    "import transformers as tr\n",
    "from transformers import pipeline\n",
    "import pandas as pd \n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise \n",
    "from itertools import cycle, islice\n",
    "import scipy as sc\n",
    "import scipy.io as sio\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import cm\n",
    "import bcolz\n",
    "import pickle\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of /Users/eghbalhosseini/MyData/semantic_proj/ failed\n",
      "\n",
      "Creation of /Users/eghbalhosseini/MyData/semantic_proj/gpt2-xl failed\n",
      "\n",
      "Creation of /Users/eghbalhosseini/MyData/semantic_proj/analysis/ failed\n",
      "\n",
      "Creation of /Users/eghbalhosseini/MyData/semantic_proj/analysis/compare_NN_empedding_with_sub_rating failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_path=\"/Users/eghbalhosseini/MyData/semantic_proj/\"\n",
    "analysis_path=\"/Users/eghbalhosseini/MyData/semantic_proj/analysis/\"\n",
    "analysis_type='compare_NN_empedding_with_sub_rating'\n",
    "model_type=\"gpt2-xl\"\n",
    "LAYER_COUNT = 48\n",
    "FEATURE_COUNT = 1024\n",
    "access_rights = 0o755\n",
    "try:\n",
    "    os.mkdir(save_path,access_rights)\n",
    "except OSError:\n",
    "    print('Creation of %s failed\\n'%save_path)\n",
    "model_path=os.path.join(save_path,model_type)\n",
    "try:\n",
    "    os.mkdir(model_path,access_rights)\n",
    "except OSError:\n",
    "    print('Creation of %s failed\\n'%model_path)\n",
    "    \n",
    "try:\n",
    "    os.mkdir(analysis_path,access_rights)\n",
    "except OSError:\n",
    "    print('Creation of %s failed\\n'%analysis_path)\n",
    "model_path=os.path.join(analysis_path,analysis_type)\n",
    "try:\n",
    "    os.mkdir(model_path,access_rights)\n",
    "except OSError:\n",
    "    print('Creation of %s failed\\n'%model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_tokenized_to_untokenized_gpt2(tokenized_sent, untokenized_sent):\n",
    "    mapping = defaultdict(list)\n",
    "    untokenized_sent_index = 0\n",
    "    tokenized_sent_index = 0\n",
    "    while (untokenized_sent_index < len(untokenized_sent) and\n",
    "        tokenized_sent_index < len(tokenized_sent)):\n",
    "      while (tokenized_sent_index + 1 < len(tokenized_sent) and\n",
    "          not(tokenized_sent[tokenized_sent_index + 1].startswith(\"Ġ\"))):\n",
    "        mapping[untokenized_sent_index].append(tokenized_sent_index)\n",
    "        tokenized_sent_index += 1\n",
    "      mapping[untokenized_sent_index].append(tokenized_sent_index)\n",
    "      untokenized_sent_index += 1\n",
    "      tokenized_sent_index += 1\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the gpt2 model \n",
    "config=GPT2Config.from_pretrained(model_type)\n",
    "config.output_hidden_states=True\n",
    "# make model from config\n",
    "model=GPT2Model(config)\n",
    "model.from_pretrained(model_type)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "# \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "domains : animal, cities ... \n",
    "\n",
    "dimensions : size, weight, ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEMANTIC_DAT_DIR='data_from_semantic_paper'\n",
    "SEMANTIC_DAT_FILE='GabeDimensions.xlsx'\n",
    "SUBJECT_RATING_FILE='rating_as_struct.mat'\n",
    "Dimensions = pd.read_excel(os.path.join(save_path,SEMANTIC_DAT_DIR,SEMANTIC_DAT_FILE),sheet_name='Dimensions')\n",
    "Domains = pd.read_excel(os.path.join(save_path,SEMANTIC_DAT_DIR,SEMANTIC_DAT_FILE),sheet_name='Domains top 25',skiprows=1)\n",
    "DIMENSIONSxDOMAINS=pd.read_excel(os.path.join(save_path,SEMANTIC_DAT_DIR,SEMANTIC_DAT_FILE),sheet_name='DIMENSIONSxDOMAINS',skiprows=0)\n",
    "subject_ratings=sio.loadmat(os.path.join(save_path,SEMANTIC_DAT_DIR,SUBJECT_RATING_FILE))\n",
    "# \n",
    "DIMENSIONSxDOMAINS=DIMENSIONSxDOMAINS.drop(DIMENSIONSxDOMAINS.index[18:],axis=0)\n",
    "DIMENSIONSxDOMAINS=DIMENSIONSxDOMAINS.drop(['Unnamed: 21','Domain Usefulness','Number of Greens'],axis=1)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_fields=[]\n",
    "for idx,x in enumerate(subject_ratings['rating_as_struct']['rating_fields'][0,0].transpose()):\n",
    "    #print((x[0][0]))\n",
    "    rating_fields.append((x[0][0]))\n",
    "words=[]\n",
    "for idx,x in enumerate(subject_ratings['rating_as_struct']['words'][0,0].transpose()):\n",
    "    word=[str(y[0].squeeze()) for y in x[0]]\n",
    "    words.append(word)\n",
    "sub_score=[]\n",
    "for idx,x in enumerate(subject_ratings['rating_as_struct']['sub_ratings'][0,0].transpose()):\n",
    "    sub_score.append(x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb5c7883ec549ccad8344084c755fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:16, 16.19s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0788933980841e38f08bc4150faa74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:32, 16.30s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cced093b20b94b1aaa7a6782e221c4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:49, 16.38s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa29a715d364265b3a15f09dd29d393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [01:05, 16.45s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded908655d6e483885ebc625f4dbc5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:22, 16.48s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b9f2b62649473d9e0f4e22b21c0d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [01:38, 16.45s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a186792710d4e9b82d6c3cb4420938b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [01:55, 16.44s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4874ed2464bc41d8a2e4a5b0456fde6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [02:12, 16.56s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b688844e414be7aa21753f45d7a85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [02:30, 16.97s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823eed7db4a743eebad11b8ef1b50856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:48, 17.39s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beac25f389d64b98bb6e90fc748f90c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [03:06, 17.56s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de11b60dc8c4836b9ff166ce6212a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [03:24, 17.76s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374f7e9b35be44c8b607814029edd833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [03:43, 18.06s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d89975a49f4b87854cbff98c7bc211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [04:02, 18.52s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348f90858a2b4902be090b66305237d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [04:21, 18.61s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe1f3fa75cb401fa787aa0a9a3d4632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [04:40, 18.71s/it]"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-eb71b08f6846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdimension\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDimensions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Dimension'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDimensions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mlineList\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mdimensions_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlineList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "for idx,domain in tqdm(enumerate(rating_fields)):\n",
    "    # 1st: get the layerwise representation for each layer of the network for each domain \n",
    "    lineList=words[idx]\n",
    "    representation_arr = np.zeros((len(lineList),2), dtype=np.object)\n",
    "    for index, line in enumerate(lineList):\n",
    "        untokenized_word = line.strip().split()\n",
    "        tokenized_word = tokenizer.tokenize(line,add_prefix_space=True)\n",
    "        untok_tok_mapping = match_tokenized_to_untokenized_gpt2(tokenized_word, untokenized_word)\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_word)\n",
    "        segment_ids = [1 for x in tokenized_word]\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segment_ids])\n",
    "        with torch.no_grad():\n",
    "            encoded_layers = model(tokens_tensor)\n",
    "            last_hidden_state=encoded_layers[0]\n",
    "            hidden_output=encoded_layers[2]\n",
    "            layer_wise_feature=[torch.stack([torch.mean(F[0,untok_tok_mapping[i][0]:untok_tok_mapping[i][-1]+1,:], dim=0) for i in range(len(untokenized_word))], dim=0) for F in hidden_output]\n",
    "            layer_wise_reperensetation= [R.view(1,*R.size()) for R in layer_wise_feature ]\n",
    "            rep_arr=[np.squeeze(x.numpy()) for x in layer_wise_reperensetation]\n",
    "            representation_arr[index,0]=' '.join(untokenized_word)\n",
    "            representation_arr[index,1]=rep_arr\n",
    "    # 2nd: get get layerwise dimensions for the corresponding domain\n",
    "    dimension=domain.split('_')[1]\n",
    "    a=np.argwhere([x.lower()==dimension for x in Dimensions['Dimension']])\n",
    "    b=Dimensions.iloc[a[0]].iloc[0][1:]\n",
    "    lineList=b\n",
    "    dimensions_arr = np.zeros((len(lineList),2), dtype=np.object)\n",
    "    for index, line in enumerate(lineList):\n",
    "        untokenized_word = line.strip().split()\n",
    "        tokenized_word = tokenizer.tokenize(line,add_prefix_space=True)\n",
    "        untok_tok_mapping = match_tokenized_to_untokenized_gpt2(tokenized_word, untokenized_word)\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_word)\n",
    "        segment_ids = [1 for x in tokenized_word]\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segment_ids])\n",
    "        with torch.no_grad():\n",
    "            encoded_layers = model(tokens_tensor)\n",
    "            last_hidden_state=encoded_layers[0]\n",
    "            hidden_output=encoded_layers[2]\n",
    "            layer_wise_feature=[torch.stack([torch.mean(F[0,untok_tok_mapping[i][0]:untok_tok_mapping[i][-1]+1,:], dim=0) for i in range(len(untokenized_word))], dim=0) for F in hidden_output]\n",
    "            layer_wise_reperensetation= [R.view(1,*R.size()) for R in layer_wise_feature ]\n",
    "            rep_arr=[np.squeeze(x.numpy()) for x in layer_wise_reperensetation]\n",
    "            dimensions_arr[index,0]=' '.join(untokenized_word)\n",
    "            dimensions_arr[index,1]=rep_arr\n",
    "        \n",
    "        # 3rd : project the data onto the representation axis\n",
    "    dim_positive=dimensions_arr[0:3]\n",
    "    dim_negative=dimensions_arr[3:]\n",
    "    sem_vec=[[np.stack(y[1])-np.stack(x[1]) for y in dim_negative] for x in dim_positive]\n",
    "    sem_vec_1=[np.stack(x) for x in sem_vec]\n",
    "    sem_vec_2=np.concatenate(sem_vec_1,axis=0)\n",
    "    sem_vec_ave=np.mean(sem_vec_2,axis=0)\n",
    "        # 4th : get the word projection onto the dimension axis across layers\n",
    "    NN_resp_projection=[]\n",
    "    NN_resp_names=[]\n",
    "    for i, x in enumerate(representation_arr):\n",
    "        NN_response=np.stack(x[1]);\n",
    "        a=np.dot(sem_vec_ave,NN_response.transpose())\n",
    "        NN_resp_projection.append(np.diag(a))\n",
    "        NN_resp_names.append(x[0])\n",
    "        \n",
    "    #5th get subject data \n",
    "    score=sub_score[idx]\n",
    "    score_ave=score.mean(1)\n",
    "    # 6th generate figures \n",
    "    NN_rep_proj_mat=np.stack(NN_resp_projection).transpose()\n",
    "    plt.figure(figsize=[20,25])\n",
    "    x=np.asarray(score_ave).reshape(-1, 1)\n",
    "    for i, y in enumerate(NN_rep_proj_mat):\n",
    "        regr = linear_model.LinearRegression()\n",
    "        y=y.reshape(-1, 1)\n",
    "        regr.fit(x, y)\n",
    "        y_pred = regr.predict(x)\n",
    "        plt.subplot(10,5,i+1)\n",
    "        plt.scatter(x,y,s=5,c=[0,0,0])\n",
    "        plt.plot(x, y_pred, color='blue', linewidth=3)\n",
    "        a=r2_score(y,y_pred)\n",
    "        s=f\"{a:.3f}\"\n",
    "        plt.title('R squared: '+s)\n",
    "    plt.savefig(model_path+'/'+model_type+'_vs_subject_'+domain+'.pdf', format='pdf',bbox_inches='tight')\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ecog_dnn] *",
   "language": "python",
   "name": "conda-env-ecog_dnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
