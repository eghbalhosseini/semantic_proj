{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "%matplotlib widget\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "\n",
    "import scipy.io as sio\n",
    "import logging\n",
    "from transformers import GPT2Model, GPT2Tokenizer, GPT2Config\n",
    "from collections import namedtuple, defaultdict\n",
    "import transformers as tr\n",
    "from transformers import pipeline\n",
    "import pandas as pd \n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise \n",
    "from itertools import cycle, islice\n",
    "import scipy as sc\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import cm\n",
    "import bcolz\n",
    "import pickle\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of /Users/eghbalhosseini/MyData/semantic_proj/ failed\n",
      "\n",
      "Creation of /Users/eghbalhosseini/MyData/semantic_proj/gpt2-xl failed\n",
      "\n",
      "Creation of /Users/eghbalhosseini/MyData/semantic_proj/analysis/ failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_path=\"/Users/eghbalhosseini/MyData/semantic_proj/\"\n",
    "analysis_path=\"/Users/eghbalhosseini/MyData/semantic_proj/analysis/\"\n",
    "analysis_type='gp2_vs_glove'\n",
    "model_type=\"gpt2-xl\"\n",
    "LAYER_COUNT = 48\n",
    "FEATURE_COUNT = 1600\n",
    "access_rights = 0o755\n",
    "try:\n",
    "    os.mkdir(save_path,access_rights)\n",
    "except OSError:\n",
    "    print('Creation of %s failed\\n'%save_path)\n",
    "model_path=os.path.join(save_path,model_type)\n",
    "try:\n",
    "    os.mkdir(model_path,access_rights)\n",
    "except OSError:\n",
    "    print('Creation of %s failed\\n'%model_path)\n",
    "    \n",
    "try:\n",
    "    os.mkdir(analysis_path,access_rights)\n",
    "except OSError:\n",
    "    print('Creation of %s failed\\n'%analysis_path)\n",
    "model_path=os.path.join(analysis_path,analysis_type)\n",
    "try:\n",
    "    os.mkdir(model_path,access_rights)\n",
    "except OSError:\n",
    "    print('Creation of %s failed\\n'%model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e0bd987e7946c08c892068315b429f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=676.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# configure the gpt2 model \n",
    "config=GPT2Config.from_pretrained(model_type)\n",
    "config.output_hidden_states=True\n",
    "# make model from config\n",
    "model=GPT2Model(config)\n",
    "model.from_pretrained(model_type)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "# \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_tokenized_to_untokenized_gpt2(tokenized_sent, untokenized_sent):\n",
    "    mapping = defaultdict(list)\n",
    "    untokenized_sent_index = 0\n",
    "    tokenized_sent_index = 0\n",
    "    while (untokenized_sent_index < len(untokenized_sent) and\n",
    "        tokenized_sent_index < len(tokenized_sent)):\n",
    "      while (tokenized_sent_index + 1 < len(tokenized_sent) and\n",
    "          not(tokenized_sent[tokenized_sent_index + 1].startswith(\"Ġ\"))):\n",
    "        mapping[untokenized_sent_index].append(tokenized_sent_index)\n",
    "        tokenized_sent_index += 1\n",
    "      mapping[untokenized_sent_index].append(tokenized_sent_index)\n",
    "      untokenized_sent_index += 1\n",
    "      tokenized_sent_index += 1\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract words from semtantic projection paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEMANTIC_DAT_DIR='data_from_semantic_paper'\n",
    "SEMANTIC_DAT_FILE='GabeDimensions.xlsx'\n",
    "Dimensions = pd.read_excel(os.path.join(save_path,SEMANTIC_DAT_DIR,SEMANTIC_DAT_FILE),sheet_name='Dimensions')\n",
    "Domains = pd.read_excel(os.path.join(save_path,SEMANTIC_DAT_DIR,SEMANTIC_DAT_FILE),sheet_name='Domains top 25',skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineList=Domains['Animals']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_arr = np.zeros((len(lineList),2), dtype=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:11,  2.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# get sentence lines \n",
    "for index, line in tqdm(enumerate(lineList)):\n",
    "    untokenized_word = line.strip().split()\n",
    "    tokenized_word = tokenizer.tokenize(line,add_prefix_space=True)\n",
    "    untok_tok_mapping = match_tokenized_to_untokenized_gpt2(tokenized_word, untokenized_word)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_word)\n",
    "    segment_ids = [1 for x in tokenized_word]\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segment_ids])\n",
    "    with torch.no_grad():\n",
    "        encoded_layers = model(tokens_tensor)\n",
    "        last_hidden_state=encoded_layers[0]\n",
    "        hidden_output=encoded_layers[2]\n",
    "        layer_wise_feature=[torch.stack([torch.mean(F[0,untok_tok_mapping[i][0]:untok_tok_mapping[i][-1]+1,:], dim=0) for i in range(len(untokenized_word))], dim=0) for F in hidden_output]\n",
    "        layer_wise_reperensetation= [R.view(1,*R.size()) for R in layer_wise_feature ]\n",
    "        rep_arr=[np.squeeze(x.numpy()) for x in layer_wise_reperensetation]\n",
    "        representation_arr[index,0]=' '.join(untokenized_word)\n",
    "        representation_arr[index,1]=rep_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[25,25])\n",
    "for i, x in enumerate(representation_arr):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    NN_response=np.stack(x[1]);\n",
    "    sort_ind=np.argsort(NN_response[-2,:])\n",
    "    plt.imshow(NN_response[:,sort_ind],aspect=25)\n",
    "    plt.title(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[25,25]);\n",
    "i=5\n",
    "NN_response_1=np.stack(representation_arr[i][1]);\n",
    "for k, x in enumerate(representation_arr):\n",
    "    plt.subplot(5,5,k+1)\n",
    "    NN_response_2=np.stack(x[1]);\n",
    "    plt.imshow(pairwise.euclidean_distances(NN_response_1,NN_response_2))\n",
    "    plt.title(representation_arr[i][0]+' vs. '+representation_arr[k][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract word embedding for the descriptors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEMANTIC_DAT_DIR='data_from_semantic_paper'\n",
    "SEMANTIC_DAT_FILE='GabeDimensions.xlsx'\n",
    "Dimensions = pd.read_excel(os.path.join(save_path,SEMANTIC_DAT_DIR,SEMANTIC_DAT_FILE),sheet_name='Dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineList=Dimensions.iloc[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions_arr = np.zeros((len(lineList),2), dtype=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:02,  2.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# get sentence lines \n",
    "for index, line in tqdm(enumerate(lineList)):\n",
    "    untokenized_word = line.strip().split()\n",
    "    tokenized_word = tokenizer.tokenize(line,add_prefix_space=True)\n",
    "    untok_tok_mapping = match_tokenized_to_untokenized_gpt2(tokenized_word, untokenized_word)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_word)\n",
    "    segment_ids = [1 for x in tokenized_word]\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segment_ids])\n",
    "    with torch.no_grad():\n",
    "        encoded_layers = model(tokens_tensor)\n",
    "        last_hidden_state=encoded_layers[0]\n",
    "        hidden_output=encoded_layers[2]\n",
    "        layer_wise_feature=[torch.stack([torch.mean(F[0,untok_tok_mapping[i][0]:untok_tok_mapping[i][-1]+1,:], dim=0) for i in range(len(untokenized_word))], dim=0) for F in hidden_output]\n",
    "        layer_wise_reperensetation= [R.view(1,*R.size()) for R in layer_wise_feature ]\n",
    "        rep_arr=[np.squeeze(x.numpy()) for x in layer_wise_reperensetation]\n",
    "        dimensions_arr[index,0]=' '.join(untokenized_word)\n",
    "        dimensions_arr[index,1]=rep_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[25,25])\n",
    "for i, x in enumerate(dimensions_arr):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    NN_response=np.stack(x[1]);\n",
    "    plt.imshow(NN_response,aspect=25)\n",
    "    plt.title(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[25,25]);\n",
    "i=1\n",
    "NN_response_1=np.stack(dimensions_arr[i][1]);\n",
    "for k, x in enumerate(dimensions_arr):\n",
    "    plt.subplot(5,5,k+1)\n",
    "    NN_response_2=np.stack(x[1]);\n",
    "    plt.imshow(pairwise.cosine_similarity(NN_response_1,NN_response_2))\n",
    "    plt.title(dimensions_arr[i][0]+' vs. '+dimensions_arr[k][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use projection to align the words along a specific dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_positive=dimensions_arr[0:3]\n",
    "dim_negative=dimensions_arr[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_vec=[[np.stack(y[1])-np.stack(x[1]) for y in dim_negative] for x in dim_positive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_vec_1=[np.stack(x) for x in sem_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_vec_2=np.concatenate(sem_vec_1,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_vec_ave=np.mean(sem_vec_2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 1600)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem_vec_ave.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8f2a53a080>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(0, 49)\n",
    "Y=X\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "plt.imshow(pairwise.cosine_similarity(sem_vec_ave,sem_vec_ave))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268f0ab362e1450abd0ad99abbf5af13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(X, Y, pairwise.cosine_similarity(sem_vec_ave,sem_vec_ave), cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get the word projection onto the dimension axis across layers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 1600)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_resp_projection=[]\n",
    "NN_resp_names=[]\n",
    "for i, x in enumerate(representation_arr):\n",
    "    NN_response=np.stack(x[1]);\n",
    "    a=np.dot(sem_vec_ave,NN_response.transpose())\n",
    "    NN_resp_projection.append(np.diag(a))\n",
    "    NN_resp_names.append(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,10])\n",
    "[plt.plot(np.arange(0,a.shape[0]),a,label=NN_resp_names[i]) for i,a in enumerate(NN_resp_projection)]\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12156862745098039, 0.4666666666666667, 0.7058823529411765, 1.0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fig = plt.figure(figsize=[10,10])\n",
    "#plt.axis([0, 10, 0, 10])\n",
    "#[[plt.text(k,m,NN_resp_names[i]) for k,m in enumerate(a)] for i,a in enumerate(NN_resp_projection)]\n",
    "#for i, x in enumerate(NN_resp_projection): \n",
    "    \n",
    "#    plt.text()\n",
    "#NN_resp_projection[1]\n",
    "tab10 = cm.get_cmap('tab10', len(NN_resp_projection))\n",
    "tab10(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[25,20])\n",
    "ax=plt.subplot(1,1,1)\n",
    "plt.plot()\n",
    "for k, m in enumerate(NN_resp_projection):\n",
    "    plt.plot(m,'.',color=tab10(k),markersize=1)\n",
    "    #plt.plot(np.asarray([k,k]),np.asarray([np.min(m),np.max(m)]))\n",
    "    #plt.show() \n",
    "    for i,a in enumerate(m):\n",
    "        ax.text(i,a,NN_resp_names[k],{'color': tab10(k), 'fontsize': 6},ha='left',va='center')\n",
    "ylims=plt.ylim()\n",
    "[plt.plot(np.asarray([i,i]),ylims,color='k',linewidth=.5) for i,_ in enumerate(NN_resp_projection[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare embedding from GPT2 against GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run glove extraction of it is not ran before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1917494it [07:04, 4518.81it/s]\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "glove_path='/Users/eghbalhosseini/MyData/semantic_proj/GLoVe/'\n",
    "vectors = bcolz.carray(np.zeros(0), rootdir=f'{glove_path}/42B.300.dat', mode='w')\n",
    "\n",
    "with open(f'{glove_path}/glove.42B.300d.txt', 'rb') as f:\n",
    "    for l in tqdm(f):\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        vectors.append(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "carray((575248201,), float64)\n",
       "  nbytes := 4.29 GB; cbytes := 4.09 GB; ratio: 1.05\n",
       "  cparams := cparams(clevel=5, shuffle=1, cname='lz4', quantize=0)\n",
       "  chunklen := 2048; chunksize: 16384; blocksize: 16384\n",
       "  rootdir := '/Users/eghbalhosseini/MyData/semantic_proj/GLoVe//42B.300.dat'\n",
       "  mode    := 'w'\n",
       "[ 0.        0.18378  -0.12123  ..., -0.090892  0.27827   0.11374 ]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = bcolz.carray(vectors[0:].reshape((-1, 300)), rootdir=f'{glove_path}/42B.300.dat', mode='w')\n",
    "vectors.flush()\n",
    "pickle.dump(words, open(f'{glove_path}/42B.300_words.pkl', 'wb'))\n",
    "pickle.dump(word2idx, open(f'{glove_path}/42B.300_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## otherwise load glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = bcolz.open(f'{glove_path}/42B.300.dat')[:]\n",
    "words = pickle.load(open(f'{glove_path}/42B.300_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(f'{glove_path}/42B.300_idx.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {w: vectors[word2idx[w]] for w in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEMANTIC_DAT_DIR='data_from_semantic_paper'\n",
    "SEMANTIC_DAT_FILE='GabeDimensions.xlsx'\n",
    "Dimensions = pd.read_excel(os.path.join(save_path,SEMANTIC_DAT_DIR,SEMANTIC_DAT_FILE),sheet_name='Dimensions')\n",
    "lineList=Dimensions.iloc[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions_arr=[glove[x] for x in lineList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_positive=dimensions_arr[0:3]\n",
    "dim_negative=dimensions_arr[3:]\n",
    "\n",
    "sem_vec=[[np.stack(y)-np.stack(x) for y in dim_negative] for x in dim_positive]\n",
    "sem_vec_1=[np.stack(x) for x in sem_vec]\n",
    "sem_vec_2=np.concatenate(sem_vec_1,axis=0)\n",
    "glove_sem_vec_ave=np.mean(sem_vec_2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEMANTIC_DAT_DIR='data_from_semantic_paper'\n",
    "SEMANTIC_DAT_FILE='GabeDimensions.xlsx'\n",
    "Dimensions = pd.read_excel(os.path.join(save_path,SEMANTIC_DAT_DIR,SEMANTIC_DAT_FILE),sheet_name='Dimensions')\n",
    "Domains = pd.read_excel(os.path.join(save_path,SEMANTIC_DAT_DIR,SEMANTIC_DAT_FILE),sheet_name='Domains top 25',skiprows=1)\n",
    "# \n",
    "lineList=Domains['Animals']\n",
    "Glove_rep_arr = np.zeros((len(lineList),2), dtype=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:00, 140371.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# get sentence lines \n",
    "for index, line in tqdm(enumerate(lineList)):\n",
    "   \n",
    "    Glove_rep_arr[index,0]=line\n",
    "    Glove_rep_arr[index,1]=glove[line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "Glove_resp_projection=[]\n",
    "Glove_resp_names=[]\n",
    "for i, x in enumerate(Glove_rep_arr):\n",
    "    Glove_response=x[1]\n",
    "    a=np.dot(glove_sem_vec_ave,Glove_response.transpose())\n",
    "    Glove_resp_projection.append(a)\n",
    "    Glove_resp_names.append(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-130-146e5d885eca>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-130-146e5d885eca>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    for i, x in enumerate(NN_rep_proj_mat):\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "NN_rep_proj_mat=np.stack(NN_resp_projection).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c33163ca0c648d1a07f0fae489ffec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[20,25])\n",
    "x=np.asarray(Glove_resp_projection).reshape(-1, 1)\n",
    "for i, y in enumerate(NN_rep_proj_mat):\n",
    "    regr = linear_model.LinearRegression()\n",
    "    y=y.reshape(-1, 1)\n",
    "    regr.fit(x, y)\n",
    "    y_pred = regr.predict(x)\n",
    "    plt.subplot(10,5,i+1)\n",
    "    plt.scatter(x,y,s=5,c=[0,0,0])\n",
    "    plt.plot(x, y_pred, color='blue', linewidth=3)\n",
    "    a=r2_score(y,y_pred)\n",
    "    s=f\"{a:.3f}\"\n",
    "    plt.title('R squared: '+s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=r2_score(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=f\"{a:.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.043'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test a sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.77it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41dca7b431c34ac890fe36e493162ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7ff9cb674e48>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(dimensions_arr[0][1][47],aspect='auto')\n",
    "plt.show()\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_path='/Users/eghbalhosseini/MyData/ecog_DNN/sentence_sampling/'\n",
    "s_dat=pd.read_pickle(os.path.join(Data_path,'sentence_data_outlier_detect.pkl'))\n",
    "s_with_all_feat_out=[]\n",
    "feat_names=['prevalence',\n",
    "             'concreteness',\n",
    "             'word length',\n",
    "             'lexical freq',\n",
    "             'arousal',\n",
    "             'valence',\n",
    "             'surprisal',\n",
    "             'sent dependency length',\n",
    "            'sent_fwprob5',\n",
    "            'sent_unigram_surp',\n",
    "            'sent_totalsurp']\n",
    "[s_with_all_feat_out.append(s_dat[x]) for x in feat_names]\n",
    "\n",
    "model_state_out=[]\n",
    "model_names=['bert_state','gpt2_state','tiny_lstm_state','ordered_neurons_state']\n",
    "[model_state_out.append(s_dat[x]) for x in model_names]\n",
    "s_string_with_all_feat_out=s_dat['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=s_string_with_all_feat_out\n",
    "dimensions_arr = np.zeros((len(sent),2), dtype=np.object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2837ef0eee4378b870ded495eee45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get sentence lines \n",
    "for index, line in tqdm(enumerate(sent)):\n",
    "    untokenized_word = line.strip().split()\n",
    "    tokenized_word = tokenizer.tokenize(line,add_prefix_space=True)\n",
    "    untok_tok_mapping = match_tokenized_to_untokenized_gpt2(tokenized_word, untokenized_word)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_word)\n",
    "    segment_ids = [1 for x in tokenized_word]\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segment_ids])\n",
    "    with torch.no_grad():\n",
    "        encoded_layers = model(tokens_tensor)\n",
    "        last_hidden_state=encoded_layers[0]\n",
    "        hidden_output=encoded_layers[2]\n",
    "        layer_wise_feature=[torch.stack([torch.mean(F[0,untok_tok_mapping[i][0]:untok_tok_mapping[i][-1]+1,:], dim=0) for i in range(len(untokenized_word))], dim=0) for F in hidden_output]\n",
    "        layer_wise_reperensetation= [R.view(1,*R.size()) for R in layer_wise_feature ]\n",
    "        rep_arr=[np.squeeze(x.numpy()) for x in layer_wise_reperensetation]\n",
    "        dimensions_arr[index,0]=' '.join(untokenized_word)\n",
    "        dimensions_arr[index,1]=rep_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/eghbalhosseini/MyData/ecog_DNN/sentence_sampling/gpt2_states_from_huggingface.npy', dimensions_arr, allow_pickle=True, fix_imports=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4629"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dimensions_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1359662,\n",
       " 4.352357,\n",
       " 5.6076064,\n",
       " 6.745062,\n",
       " 8.518669,\n",
       " 9.29471,\n",
       " 9.802319,\n",
       " 10.263653,\n",
       " 10.432583,\n",
       " 11.664098,\n",
       " 12.359014,\n",
       " 13.67273,\n",
       " 14.751403,\n",
       " 17.016434,\n",
       " 17.362995,\n",
       " 17.758003,\n",
       " 21.52797,\n",
       " 22.674604,\n",
       " 22.965948,\n",
       " 22.942162,\n",
       " 23.33326,\n",
       " 24.13773,\n",
       " 23.26604,\n",
       " 22.043873,\n",
       " 20.925617,\n",
       " 21.43827,\n",
       " 21.20632,\n",
       " 21.059341,\n",
       " 21.572973,\n",
       " 20.881594,\n",
       " 22.675613,\n",
       " 22.949026,\n",
       " 23.027697,\n",
       " 22.701693,\n",
       " 22.43643,\n",
       " 22.220613,\n",
       " 22.405735,\n",
       " 23.844685,\n",
       " 26.139372,\n",
       " 26.659834,\n",
       " 25.98666,\n",
       " 27.897467,\n",
       " 26.242613,\n",
       " 26.889782,\n",
       " 28.600185,\n",
       " 29.764103,\n",
       " 29.084406,\n",
       " 30.561567,\n",
       " 3.929636]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.max(x) for x in dimensions_arr[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 1600)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(gpt2_state_data[1],gpt2_state_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ea09a3be16490f81f7b9d5f6feb13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "for x in dimensions_arr[0][1]: \n",
    "    plt.plot(np.matmul(x,gpt2_state_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-arch",
   "language": "python",
   "name": "arch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
