{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline \n",
    "import scipy.io as sio\n",
    "import logging\n",
    "from transformers import GPT2Model, GPT2Tokenizer, GPT2Config\n",
    "from collections import namedtuple, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=\"/Users/eghbalhosseini/MyData/semantic_proj/\"\n",
    "model_type=\"gpt2-xl\"\n",
    "LAYER_COUNT = 48\n",
    "FEATURE_COUNT = 1024\n",
    "access_rights = 0o755\n",
    "try:\n",
    "    os.mkdir(save_path,access_rights)\n",
    "except OSError:\n",
    "    print('Creation of %s failed\\n'%save_path)\n",
    "model_path=os.path.join(save_path,model_type)\n",
    "try:\n",
    "    os.mkdir(model_path,access_rights)\n",
    "except OSError:\n",
    "    print('Creation of %s failed\\n'%model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5c14496f1f44679c65f6bb3569ce06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=670.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# configure the gpt2 model \n",
    "config=GPT2Config.from_pretrained(model_type)\n",
    "config.output_hidden_states=True\n",
    "# make model from config\n",
    "model=GPT2Model(config)\n",
    "model.from_pretrained(model_type)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "# \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_tokenized_to_untokenized_gpt2(tokenized_sent, untokenized_sent):\n",
    "    mapping = defaultdict(list)\n",
    "    untokenized_sent_index = 0\n",
    "    tokenized_sent_index = 0\n",
    "    while (untokenized_sent_index < len(untokenized_sent) and\n",
    "        tokenized_sent_index < len(tokenized_sent)):\n",
    "      while (tokenized_sent_index + 1 < len(tokenized_sent) and\n",
    "          not(tokenized_sent[tokenized_sent_index + 1].startswith(\"Ġ\"))):\n",
    "        mapping[untokenized_sent_index].append(tokenized_sent_index)\n",
    "        tokenized_sent_index += 1\n",
    "      mapping[untokenized_sent_index].append(tokenized_sent_index)\n",
    "      untokenized_sent_index += 1\n",
    "      tokenized_sent_index += 1\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a few example sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineList=['tiger is more dangerous than dolphin',\n",
    "          'elephant is bigger than tiger',\n",
    "         'chicken is smaller than elephant',\n",
    "         'tiger faster than elephant',\n",
    "         'turtle is slower than dolphin',\n",
    "         'elephant is heavier than tiger']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_arr = np.zeros((len(lineList),2), dtype=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:02,  2.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# get sentence lines \n",
    "for index, line in tqdm(enumerate(lineList)):\n",
    "    untokenized_sent = line.strip().split()\n",
    "    tokenized_sent = tokenizer.tokenize(line,add_prefix_space=True)\n",
    "    untok_tok_mapping = match_tokenized_to_untokenized_gpt2(tokenized_sent, untokenized_sent)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_sent)\n",
    "    segment_ids = [1 for x in tokenized_sent]\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segment_ids])\n",
    "    with torch.no_grad():\n",
    "        encoded_layers = model(tokens_tensor)\n",
    "        last_hidden_state=encoded_layers[0]\n",
    "        hidden_output=encoded_layers[2]\n",
    "        layer_wise_feature=[torch.stack([torch.mean(F[0,untok_tok_mapping[i][0]:untok_tok_mapping[i][-1]+1,:], dim=0) for i in range(len(untokenized_sent))], dim=0) for F in hidden_output]\n",
    "        layer_wise_reperensetation= [R.view(1,*R.size()) for R in layer_wise_feature ]\n",
    "        rep_arr=[np.squeeze(x.numpy()) for x in layer_wise_reperensetation]\n",
    "        representation_arr[index,0]=' '.join(untokenized_sent)\n",
    "        representation_arr[index,1]=rep_arr\n",
    "sio.savemat(os.path.join(model_path, 'sentence_representation_cells_all_layers.mat'), {'sentence_representation_arr':representation_arr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ecog_dnn] *",
   "language": "python",
   "name": "conda-env-ecog_dnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
