{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "%matplotlib widget\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "\n",
    "import scipy.io as sio\n",
    "import logging\n",
    "from transformers import GPT2Model, GPT2Tokenizer, GPT2Config\n",
    "from collections import namedtuple, defaultdict\n",
    "import transformers as tr\n",
    "from transformers import pipeline\n",
    "import pandas as pd \n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise \n",
    "from itertools import cycle, islice\n",
    "import scipy as sc\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import cm\n",
    "import bcolz\n",
    "import pickle\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of /Users/eghbalhosseini/MyData/semantic_proj/ failed\n",
      "\n",
      "Creation of /Users/eghbalhosseini/MyData/semantic_proj/gpt2-xl failed\n",
      "\n",
      "Creation of /Users/eghbalhosseini/MyData/semantic_proj/analysis/ failed\n",
      "\n",
      "Creation of /Users/eghbalhosseini/MyData/semantic_proj/analysis/compare_NN_empedding_with_glove failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_path=\"/Users/eghbalhosseini/MyData/semantic_proj/\"\n",
    "analysis_path=\"/Users/eghbalhosseini/MyData/semantic_proj/analysis/\"\n",
    "analysis_type='compare_NN_empedding_with_glove'\n",
    "model_type=\"gpt2-xl\"\n",
    "LAYER_COUNT = 48\n",
    "FEATURE_COUNT = 1024\n",
    "access_rights = 0o755\n",
    "try:\n",
    "    os.mkdir(save_path,access_rights)\n",
    "except OSError:\n",
    "    print('Creation of %s failed\\n'%save_path)\n",
    "model_path=os.path.join(save_path,model_type)\n",
    "try:\n",
    "    os.mkdir(model_path,access_rights)\n",
    "except OSError:\n",
    "    print('Creation of %s failed\\n'%model_path)\n",
    "    \n",
    "try:\n",
    "    os.mkdir(analysis_path,access_rights)\n",
    "except OSError:\n",
    "    print('Creation of %s failed\\n'%analysis_path)\n",
    "model_path=os.path.join(analysis_path,analysis_type)\n",
    "try:\n",
    "    os.mkdir(model_path,access_rights)\n",
    "except OSError:\n",
    "    print('Creation of %s failed\\n'%model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_tokenized_to_untokenized_gpt2(tokenized_sent, untokenized_sent):\n",
    "    mapping = defaultdict(list)\n",
    "    untokenized_sent_index = 0\n",
    "    tokenized_sent_index = 0\n",
    "    while (untokenized_sent_index < len(untokenized_sent) and\n",
    "        tokenized_sent_index < len(tokenized_sent)):\n",
    "      while (tokenized_sent_index + 1 < len(tokenized_sent) and\n",
    "          not(tokenized_sent[tokenized_sent_index + 1].startswith(\"Ġ\"))):\n",
    "        mapping[untokenized_sent_index].append(tokenized_sent_index)\n",
    "        tokenized_sent_index += 1\n",
    "      mapping[untokenized_sent_index].append(tokenized_sent_index)\n",
    "      untokenized_sent_index += 1\n",
    "      tokenized_sent_index += 1\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the gpt2 model \n",
    "config=GPT2Config.from_pretrained(model_type)\n",
    "config.output_hidden_states=True\n",
    "# make model from config\n",
    "model=GPT2Model(config)\n",
    "model.from_pretrained(model_type)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "# \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEMANTIC_DAT_DIR='data_from_semantic_paper'\n",
    "SEMANTIC_DAT_FILE='GabeDimensions.xlsx'\n",
    "Dimensions = pd.read_excel(os.path.join(save_path,SEMANTIC_DAT_DIR,SEMANTIC_DAT_FILE),sheet_name='Dimensions')\n",
    "Domains = pd.read_excel(os.path.join(save_path,SEMANTIC_DAT_DIR,SEMANTIC_DAT_FILE),sheet_name='Domains top 25',skiprows=1)\n",
    "DIMENSIONSxDOMAINS=pd.read_excel(os.path.join(save_path,SEMANTIC_DAT_DIR,SEMANTIC_DAT_FILE),sheet_name='DIMENSIONSxDOMAINS',skiprows=0)\n",
    "# \n",
    "DIMENSIONSxDOMAINS=DIMENSIONSxDOMAINS.drop(DIMENSIONSxDOMAINS.index[18:],axis=0)\n",
    "DIMENSIONSxDOMAINS=DIMENSIONSxDOMAINS.drop(['Unnamed: 21','Domain Usefulness','Number of Greens'],axis=1)\n",
    "# \n",
    "glove_path='/Users/eghbalhosseini/MyData/semantic_proj/GLoVe/'\n",
    "vectors = bcolz.open(f'{glove_path}/42B.300.dat')[:]\n",
    "words = pickle.load(open(f'{glove_path}/42B.300_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(f'{glove_path}/42B.300_idx.pkl', 'rb'))\n",
    "glove = {w: vectors[word2idx[w]] for w in words}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain in tqdm(Domains.columns[:-1]):\n",
    "    # 1st: get the layerwise representation for each layer of the network\n",
    "    lineList=Domains[domain]\n",
    "    corr_entry=[type(x)==str for x in lineList]\n",
    "    lineList=lineList[corr_entry]\n",
    "    representation_arr = np.zeros((len(lineList),2), dtype=np.object)\n",
    "    for index, line in enumerate(lineList):\n",
    "        untokenized_word = line.strip().split()\n",
    "        tokenized_word = tokenizer.tokenize(line,add_prefix_space=True)\n",
    "        untok_tok_mapping = match_tokenized_to_untokenized_gpt2(tokenized_word, untokenized_word)\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_word)\n",
    "        segment_ids = [1 for x in tokenized_word]\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segment_ids])\n",
    "        with torch.no_grad():\n",
    "            encoded_layers = model(tokens_tensor)\n",
    "            last_hidden_state=encoded_layers[0]\n",
    "            hidden_output=encoded_layers[2]\n",
    "            layer_wise_feature=[torch.stack([torch.mean(F[0,untok_tok_mapping[i][0]:untok_tok_mapping[i][-1]+1,:], dim=0) for i in range(len(untokenized_word))], dim=0) for F in hidden_output]\n",
    "            layer_wise_reperensetation= [R.view(1,*R.size()) for R in layer_wise_feature ]\n",
    "            rep_arr=[np.squeeze(x.numpy()) for x in layer_wise_reperensetation]\n",
    "            representation_arr[index,0]=' '.join(untokenized_word)\n",
    "            representation_arr[index,1]=rep_arr\n",
    "    # 2nd: get domains for which the dimension is reasonable\n",
    "    \n",
    "    a=np.argwhere(DIMENSIONSxDOMAINS['Unnamed: 0']==domain)\n",
    "    if a.size>0:\n",
    "        lineList=DIMENSIONSxDOMAINS.iloc[a.squeeze()][0:]\n",
    "        # threshold\n",
    "        lineList==3\n",
    "        feature_domain_names=DIMENSIONSxDOMAINS.columns[np.argwhere(lineList==3)]\n",
    "        # 3rd extract feature domains and project for both NN and glove\n",
    "        for k,name in enumerate(feature_domain_names):\n",
    "            feat=np.argwhere(Dimensions.Dimension==name)\n",
    "            b=Dimensions.iloc[feat[0]].iloc[0][1:]\n",
    "            lineList=b\n",
    "            dimensions_arr = np.zeros((len(lineList),2), dtype=np.object)\n",
    "            for index, line in enumerate(lineList):\n",
    "                untokenized_word = line.strip().split()\n",
    "                tokenized_word = tokenizer.tokenize(line,add_prefix_space=True)\n",
    "                untok_tok_mapping = match_tokenized_to_untokenized_gpt2(tokenized_word, untokenized_word)\n",
    "                indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_word)\n",
    "                segment_ids = [1 for x in tokenized_word]\n",
    "                tokens_tensor = torch.tensor([indexed_tokens])\n",
    "                segments_tensors = torch.tensor([segment_ids])\n",
    "                with torch.no_grad():\n",
    "                    encoded_layers = model(tokens_tensor)\n",
    "                    last_hidden_state=encoded_layers[0]\n",
    "                    hidden_output=encoded_layers[2]\n",
    "                    layer_wise_feature=[torch.stack([torch.mean(F[0,untok_tok_mapping[i][0]:untok_tok_mapping[i][-1]+1,:], dim=0) for i in range(len(untokenized_word))], dim=0) for F in hidden_output]\n",
    "                    layer_wise_reperensetation= [R.view(1,*R.size()) for R in layer_wise_feature ]\n",
    "                    rep_arr=[np.squeeze(x.numpy()) for x in layer_wise_reperensetation]\n",
    "                    dimensions_arr[index,0]=' '.join(untokenized_word)\n",
    "                    dimensions_arr[index,1]=rep_arr\n",
    "\n",
    "        # 4th : project the data onto the representation axis\n",
    "            dim_positive=dimensions_arr[0:3]\n",
    "            dim_negative=dimensions_arr[3:]\n",
    "            sem_vec=[[np.stack(y[1])-np.stack(x[1]) for y in dim_negative] for x in dim_positive]\n",
    "            sem_vec_1=[np.stack(x) for x in sem_vec]\n",
    "            sem_vec_2=np.concatenate(sem_vec_1,axis=0)\n",
    "            sem_vec_ave=np.mean(sem_vec_2,axis=0)\n",
    "        # 5th : get the word projection onto the dimension axis across layers\n",
    "            NN_resp_projection=[]\n",
    "            NN_resp_names=[]\n",
    "            for i, x in enumerate(representation_arr):\n",
    "                NN_response=np.stack(x[1]);\n",
    "                a=np.dot(sem_vec_ave,NN_response.transpose())\n",
    "                NN_resp_projection.append(np.diag(a))\n",
    "                NN_resp_names.append(x[0])\n",
    "\n",
    "        # 6th get glove embedding\n",
    "        # 6.1 get dim vectors\n",
    "            dimensions_arr=[glove[x] for x in lineList]\n",
    "            dim_positive=dimensions_arr[0:3]\n",
    "            dim_negative=dimensions_arr[3:]\n",
    "\n",
    "            sem_vec=[[np.stack(y)-np.stack(x) for y in dim_negative] for x in dim_positive]\n",
    "            sem_vec_1=[np.stack(x) for x in sem_vec]\n",
    "            sem_vec_2=np.concatenate(sem_vec_1,axis=0)\n",
    "            glove_sem_vec_ave=np.mean(sem_vec_2,axis=0)\n",
    "        # 6.2 get domains\n",
    "            lineList=Domains[domain]\n",
    "            corr_entry=[type(x)==str for x in lineList]\n",
    "            lineList=lineList[corr_entry]\n",
    "            Glove_rep_arr = np.zeros((len(lineList),2), dtype=np.object)\n",
    "            for index, line in enumerate(lineList):\n",
    "                Glove_rep_arr[index,0]=line\n",
    "                Glove_rep_arr[index,1]=glove[line]\n",
    "        # 6.3 projection\n",
    "            Glove_resp_projection=[]\n",
    "            Glove_resp_names=[]\n",
    "            for i, x in enumerate(Glove_rep_arr):\n",
    "                Glove_response=x[1]\n",
    "                a=np.dot(glove_sem_vec_ave,Glove_response.transpose())\n",
    "                Glove_resp_projection.append(a)\n",
    "                Glove_resp_names.append(x[0])\n",
    "            NN_rep_proj_mat=np.stack(NN_resp_projection).transpose()\n",
    "            plt.figure(figsize=[20,25])\n",
    "            x=np.asarray(Glove_resp_projection).reshape(-1, 1)\n",
    "            for i, y in enumerate(NN_rep_proj_mat):\n",
    "                regr = linear_model.LinearRegression()\n",
    "                y=y.reshape(-1, 1)\n",
    "                regr.fit(x, y)\n",
    "                y_pred = regr.predict(x)\n",
    "                plt.subplot(10,5,i+1)\n",
    "                plt.scatter(x,y,s=5,c=[0,0,0])\n",
    "                plt.plot(x, y_pred, color='blue', linewidth=3)\n",
    "                a=r2_score(y,y_pred)\n",
    "                s=f\"{a:.3f}\"\n",
    "                plt.title('R squared: '+s)\n",
    "            plt.savefig(model_path+'/'+model_type+'_vs_GloVe_'+domain+'_'+name+'.pdf', format='pdf',bbox_inches='tight')\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                may\n",
       "1          christmas\n",
       "2              march\n",
       "3               june\n",
       "4              april\n",
       "5               july\n",
       "6             august\n",
       "7          september\n",
       "8            october\n",
       "9           november\n",
       "10      thanksgiving\n",
       "11          december\n",
       "12         halloween\n",
       "13           january\n",
       "14            easter\n",
       "15          february\n",
       "16              lent\n",
       "17          hanukkah\n",
       "18            advent\n",
       "19          passover\n",
       "20           ramadan\n",
       "21    valentines-day\n",
       "Name: Calendar, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Size</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Time</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Loudness</th>\n",
       "      <th>Location</th>\n",
       "      <th>Danger</th>\n",
       "      <th>Wetness</th>\n",
       "      <th>...</th>\n",
       "      <th>Wealth</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Taste</th>\n",
       "      <th>Intelligence</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Quality</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>Political</th>\n",
       "      <th>Religiosity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, Size, Weight, Temperature, Time, Speed, Loudness, Location, Danger, Wetness, Cost, Wealth, Gender, Taste, Intelligence, Emotion, Quality, Valence, Arousal, Political, Religiosity]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 21 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_domain_names=DIMENSIONSxDOMAINS.columns[np.argwhere(lineList==3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_dnn",
   "language": "python",
   "name": "dnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
